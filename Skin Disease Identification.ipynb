{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92e48bf3-6857-4908-806d-c0b20f836bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for image processing, data manipulation, model building, training, evaluation, and saving\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dff48b2-af01-4769-9e7e-2a55fc36cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the dataset and define the target image size\n",
    "data_dir = 'Skin_Disease_Dataset'\n",
    "img_size = 64\n",
    "\n",
    "# Initialize empty lists to store image data and corresponding labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Loop through each disease category folder in the dataset directory\n",
    "for label in os.listdir(data_dir):\n",
    "    folder_path = os.path.join(data_dir, label)\n",
    "    # Loop through each image file in the category folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        # Process only valid image file formats\n",
    "        if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img = cv2.imread(img_path)  # Read the image\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, (img_size, img_size))  # Resize image to target size\n",
    "                images.append(img)  # Append image data to list\n",
    "                labels.append(label)  # Append corresponding label\n",
    "            else:\n",
    "                print(f\"Warning: Could not read image {img_path}, skipping.\")  # Handle unreadable images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "892ebd7e-527a-4f27-8ce8-4be18a1f0b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images loaded: 26945\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of images to a NumPy array and normalize pixel values to range [0, 1]\n",
    "images = np.array(images, dtype='float32') / 255.0\n",
    "\n",
    "# Convert the list of labels to a NumPy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Print the total number of images loaded\n",
    "print(f\"Total images loaded: {len(images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c3b7419-68e4-4d76-96c4-35c0bccc6658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Convert string labels (e.g., 'Acne', 'Eczema') to numeric form\n",
    "labels_encoded = le.fit_transform(labels)\n",
    "\n",
    "# Convert numeric labels to one-hot encoded format for training\n",
    "labels_categorical = to_categorical(labels_encoded)\n",
    "\n",
    "# Save the label encoder to a file for future use during prediction\n",
    "with open(\"labels.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c61577a7-dc93-47ac-8d8e-ea60c86ada55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "# 80% of data will be used for training and 20% for testing\n",
    "# random_state=42 ensures reproducibility of the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels_categorical, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9fcbb76-949d-4a03-88a0-18dfd852534e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,605,760</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │       \u001b[38;5;34m1,605,760\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │             \u001b[38;5;34m645\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,625,797</span> (6.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,625,797\u001b[0m (6.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,625,797</span> (6.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,625,797\u001b[0m (6.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build a Convolutional Neural Network (CNN) model using Keras Sequential API\n",
    "model = Sequential([\n",
    "    # First convolutional layer with 32 filters, 3x3 kernel, ReLU activation\n",
    "    # Input shape corresponds to the image size and 3 color channels (RGB)\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(img_size, img_size, 3)),\n",
    "    MaxPooling2D(2,2),  # Downsampling with 2x2 max pooling to reduce spatial dimensions\n",
    "\n",
    "    # Second convolutional layer with 64 filters and ReLU activation\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),  # Another max pooling layer to further reduce size\n",
    "\n",
    "    Flatten(),  # Flatten 3D feature maps to 1D feature vector for dense layers\n",
    "    Dense(128, activation='relu'),  # Fully connected layer with 128 neurons and ReLU activation\n",
    "    Dropout(0.5),  # Dropout layer with 50% rate to reduce overfitting\n",
    "\n",
    "    # Output layer with neurons equal to number of classes\n",
    "    # Softmax activation outputs class probabilities\n",
    "    Dense(len(le.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer, categorical cross-entropy loss, and accuracy metric\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print a summary of the model architecture and parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe5dd42-cd07-464b-9a5b-4f5384272c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m674/674\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 46ms/step - accuracy: 0.5488 - loss: 0.9756 - val_accuracy: 0.7669 - val_loss: 0.5106\n",
      "Epoch 2/15\n",
      "\u001b[1m674/674\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 45ms/step - accuracy: 0.7486 - loss: 0.5614 - val_accuracy: 0.8135 - val_loss: 0.4489\n",
      "Epoch 3/15\n",
      "\u001b[1m674/674\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 44ms/step - accuracy: 0.7946 - loss: 0.4483 - val_accuracy: 0.8406 - val_loss: 0.3705\n",
      "Epoch 4/15\n",
      "\u001b[1m674/674\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 45ms/step - accuracy: 0.8335 - loss: 0.3897 - val_accuracy: 0.8605 - val_loss: 0.3125\n",
      "Epoch 5/15\n",
      "\u001b[1m674/674\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 43ms/step - accuracy: 0.8533 - loss: 0.3547 - val_accuracy: 0.8901 - val_loss: 0.2613\n",
      "Epoch 6/15\n",
      "\u001b[1m674/674\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 50ms/step - accuracy: 0.8737 - loss: 0.3037 - val_accuracy: 0.9011 - val_loss: 0.2441\n",
      "Epoch 7/15\n",
      "\u001b[1m674/674\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 47ms/step - accuracy: 0.8847 - loss: 0.2836 - val_accuracy: 0.8850 - val_loss: 0.2839\n",
      "Epoch 8/15\n",
      "\u001b[1m674/674\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 44ms/step - accuracy: 0.8786 - loss: 0.2791 - val_accuracy: 0.9111 - val_loss: 0.2221\n",
      "Epoch 9/15\n",
      "\u001b[1m674/674\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 45ms/step - accuracy: 0.9033 - loss: 0.2331 - val_accuracy: 0.9098 - val_loss: 0.2173\n",
      "Epoch 10/15\n",
      "\u001b[1m674/674\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 46ms/step - accuracy: 0.8992 - loss: 0.2357 - val_accuracy: 0.9098 - val_loss: 0.2476\n",
      "Epoch 11/15\n",
      "\u001b[1m674/674\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 46ms/step - accuracy: 0.9001 - loss: 0.2265 - val_accuracy: 0.8935 - val_loss: 0.2628\n",
      "Epoch 12/15\n",
      "\u001b[1m674/674\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 43ms/step - accuracy: 0.9065 - loss: 0.2330 - val_accuracy: 0.9139 - val_loss: 0.2104\n",
      "Epoch 13/15\n",
      "\u001b[1m674/674\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 43ms/step - accuracy: 0.9220 - loss: 0.1900 - val_accuracy: 0.9148 - val_loss: 0.2253\n",
      "Epoch 14/15\n",
      "\u001b[1m674/674\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 43ms/step - accuracy: 0.9217 - loss: 0.1840 - val_accuracy: 0.9174 - val_loss: 0.2142\n",
      "Epoch 15/15\n",
      "\u001b[1m674/674\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 44ms/step - accuracy: 0.9236 - loss: 0.1805 - val_accuracy: 0.9184 - val_loss: 0.2121\n"
     ]
    }
   ],
   "source": [
    "# Train the CNN model on training data\n",
    "# Validate the model on the test data after each epoch to monitor performance\n",
    "# epochs=15 means the model will see the entire training set 15 times\n",
    "# batch_size=32 means training will happen in mini-batches of 32 images for efficient learning\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b80877a9-fb76-462d-892e-99037f35d94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9144 - loss: 0.2135\n",
      "Test Accuracy: 91.84%\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Acne       0.97      0.95      0.96      2036\n",
      "      Eczema       0.64      0.60      0.62       332\n",
      "    Melanoma       0.97      0.95      0.96       630\n",
      "   Psoriasis       0.66      0.70      0.68       432\n",
      "     Rosacea       0.95      0.97      0.96      1959\n",
      "\n",
      "    accuracy                           0.92      5389\n",
      "   macro avg       0.84      0.84      0.84      5389\n",
      "weighted avg       0.92      0.92      0.92      5389\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the trained model's loss and accuracy on the test dataset\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")  # Print test accuracy as percentage\n",
    "\n",
    "# Predict class probabilities for all test images\n",
    "y_pred_probs = model.predict(X_test)\n",
    "\n",
    "# Convert predicted probabilities to class labels by selecting the index with highest probability\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Get true class labels from one-hot encoded y_test by selecting the index of '1'\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Generate and print detailed classification report with precision, recall, f1-score for each class\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1110e941-146f-4408-ba42-65fca2cbf9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained CNN model in Keras (.keras) format for later use or deployment\n",
    "model.save(\"skin_disease_identification_cnn_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3eeec9d-c786-42b2-804c-1b04b5252c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img_path):\n",
    "    # Read the image from the given path\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        # Handle case when image path is invalid or file is missing\n",
    "        print(\"Image not found.\")\n",
    "        return\n",
    "    \n",
    "    # Resize the image to match model input size and normalize pixel values\n",
    "    img = cv2.resize(img, (img_size, img_size)) / 255.0\n",
    "    \n",
    "    # Add batch dimension since model expects input shape (1, img_size, img_size, 3)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    # Get prediction probabilities from the model\n",
    "    prediction = model.predict(img)\n",
    "    \n",
    "    # Find the index of the highest probability (predicted class)\n",
    "    predicted_index = np.argmax(prediction)\n",
    "    \n",
    "    # Decode the label back to original class name\n",
    "    predicted_class = le.inverse_transform([predicted_index])[0]\n",
    "\n",
    "    # Print predicted class\n",
    "    print(f\"Predicted disease: {predicted_class}\")\n",
    "    \n",
    "    # Print confidence scores for each class to understand model certainty\n",
    "    print(\"Confidence scores:\")\n",
    "    for i, score in enumerate(prediction[0]):\n",
    "        print(f\"  {le.inverse_transform([i])[0]}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c027c43f-4514-4fea-80e7-42abac05c837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "Predicted disease: Rosacea\n",
      "Confidence scores:\n",
      "  Acne: 0.0216\n",
      "  Eczema: 0.0000\n",
      "  Melanoma: 0.0000\n",
      "  Psoriasis: 0.0000\n",
      "  Rosacea: 0.9784\n"
     ]
    }
   ],
   "source": [
    "# Call the prediction function on a sample image\n",
    "predict_image('Skin_Disease_Dataset/Rosacea/aug_0_4.jpg')\n",
    "# This will print the predicted skin disease class and confidence scores for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ab4dd-aa61-42ab-ab04-d71b46ed4865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
